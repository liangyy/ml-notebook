<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Chapter 2: 2.1 - 2.5</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="esl.html">ESL reading</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/liangyy/ml-notebook">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Chapter 2: 2.1 - 2.5</h1>
<h4 class="date">24 October, 2020</h4>

</div>


<p><span class="math display">\[
\newcommand{\epe}{\text{EPE}}
\newcommand{\E}{\text{E}}
\newcommand{\mse}{\text{MSE}}
\newcommand{\tdata}{\mathcal{T}}
\newcommand{\bias}{\text{Bias}}
\newcommand{\var}{\text{Var}}
\]</span></p>
<div id="about" class="section level1">
<h1><span class="header-section-number">1</span> About</h1>
<p>Here we mostly discussed linear model and k-nearest neighbor as two extreme machine learning models. Depending on the data generation process, one may perform better than the other.</p>
</div>
<div id="knn" class="section level1">
<h1><span class="header-section-number">2</span> knn</h1>
<p>For the knn approach, it achieves better and better training performance as k decreases. And the effective number of parameters in knn is <span class="math inline">\(N / k\)</span> which corresponding to, roughly, the number of partitions that the sample space could have (as we need to memorize the result for each partition).</p>
<p>Some potential enhancements of knn are:</p>
<ul>
<li>Use kernel method to weight samples rather than using the 0/1 weight for nearest neighbors.</li>
<li>For high dimensional data, we may want to carefully define distance metric so that it can emphasize certain dim.</li>
</ul>
<p><strong>Challenge</strong>:</p>
<ul>
<li>In high dimensional data space, it is hard to have enough “near neighbors”. And because of this, the performance may be bad.</li>
<li>If we know the underlying pattern of data, we can build models with smaller bias and variance.</li>
</ul>
</div>
<div id="linear-model" class="section level1">
<h1><span class="header-section-number">3</span> Linear model</h1>
<p>For least squares loss (most widely used one since it is easy analytically), the solution is <span class="math inline">\((X&#39;X)^{-1}(X&#39;y)\)</span>. It creates linear decision boundary for classification problem (for cutoff = 0.5, the boundary is <span class="math inline">\(X ~ s.t. ~X\beta = 0\)</span> for any <span class="math inline">\(\beta\)</span>).</p>
<p>Linear model may over-simplify things and here are some potential enhancements:</p>
<ul>
<li>Local regression using local weights.</li>
<li>Linear model under expanded basis.</li>
<li>Add non-linear transformation, <em>e.g.</em> neural network.</li>
</ul>
</div>
<div id="statistical-decision-theory" class="section level1">
<h1><span class="header-section-number">4</span> Statistical decision theory</h1>
<p>First, we define loss function <span class="math inline">\(L(Y, f(X))\)</span> where <span class="math inline">\(f\)</span> is a prediction model. For example, we use squared error loss: <span class="math inline">\(L(Y, f(X)) = (Y - f(X))^2\)</span>. And then we can define expected loss of the model <span class="math inline">\(f\)</span>.</p>
<p><span class="math display">\[\begin{aligned}
  \epe(f) &amp;= \E(L(Y, f(X))) \\
          &amp;= \E(Y - f(X))^2 \\
          &amp;= \int [y - f(x)]^2 \Pr(dx, dy) 
\end{aligned}\]</span></p>
<p>And we can always use the <strong>trick</strong> to do the expectation is two steps:</p>
<p><span class="math display">\[\begin{aligned}
  \epe(f) &amp;= \E_X \E_{Y | X} ([Y - f(X)]^2 | X)
\end{aligned}\]</span></p>
<p>So if we want to minimize <span class="math inline">\(\epe(f)\)</span>, we can do it for each <span class="math inline">\(X\)</span> value separately. Consider an extreme case where <span class="math inline">\(Y\)</span> has no extra noise given <span class="math inline">\(X\)</span>, then ideally, we want <span class="math inline">\(f(X) = Y\)</span> which gives <span class="math inline">\(\epe(f) = 0\)</span>. Following this idea, we have minimizer <span class="math inline">\(f\)</span> being <span class="math inline">\(f(x) = \E(Y | X = x)\)</span> which is <strong>the ideal model to learn</strong>.</p>
<p>In this view, we can think of linear model and knn as ways to learn the <span class="math inline">\(\E(Y | X = x)\)</span>. For knn, it learns it in completely non-parametric way. And for linear model, we assume that <span class="math inline">\(Y = X\beta + \text{noise}\)</span>.</p>
<p>If <span class="math inline">\(k / N \rightarrow 0\)</span>, <span class="math inline">\(\hat{f}(x) \rightarrow \E(Y|X = x)\)</span> for knn <span class="math inline">\(f\)</span>. But things get harder when <span class="math inline">\(p\)</span> (data dimension) is high since we need lot more neighbors to have a good estimate of <span class="math inline">\(\E(Y | X = x)\)</span>. And for small sample size, if we know the data follow some model, we can use these information to increase the predictive power. To extend linear model to make it more flexible, we can do <span class="math inline">\(f(X) = \sum_{j = 1}^p f_j(X_j)\)</span> where <span class="math inline">\(f_j\)</span> could be learned in non-parametric way but only 1-d.</p>
<p>What if other loss function? For <span class="math inline">\(L_1\)</span> loss <span class="math inline">\(|Y - f(X)|\)</span>, the corresponding <strong>ideal model to learn</strong> is <span class="math inline">\(\text{median}(Y | X = x)\)</span> (it is still point-wise but median instead of mean). For classification problem, the loss can be the number of mis-classification. And it turns out that, in this case, <strong>the ideal model to learn</strong> is <span class="math inline">\(\max_g \Pr(g | X = x)\)</span>, <em>i.e.</em> the model popular label given <span class="math inline">\(X = x\)</span>.</p>
</div>
<div id="about-mse" class="section level1">
<h1><span class="header-section-number">5</span> About MSE</h1>
<p>Let’s consider a special case where we have a error-free <span class="math inline">\(y_0\)</span>. We define mean squared error (MSE) as <span class="math inline">\(\mse(x_0) = \E_{\tdata}[ f(x_0) - \hat{y}_0 ]^2\)</span>. And we do the typical trick</p>
<p><span class="math display">\[\begin{aligned}
  mse(x_0) &amp;:= \E_{\tdata}[ f(x_0) - \hat{y}_0 ]^2 \\
           &amp;= \E_{\tdata}[ f(x_0) - \E_\tdata(\hat{y}_0) + \E_\tdata(\hat{y}_0) - \hat{y}_0 ]^2 \\
           &amp;= \E_{\tdata}[ f(x_0) - \E_\tdata(\hat{y}_0)]^2 + \E_{\tdata}[\E_\tdata(\hat{y}_0) - \hat{y}_0 ]^2 - \E_{\tdata}[ f(x_0) - \E_\tdata(\hat{y}_0)] \underbrace{\E_{\tdata}[\E_\tdata(\hat{y}_0) - \hat{y}_0 ]}_{0} \\
           &amp;= \bias_\tdata^2(\hat{y}_0) + \var_\tdata(\hat{y}_0)
\end{aligned}\]</span></p>
<p>Note that the bias and variance is taken under the training data. It captures the average behavior when we train the model using many copies of the training data.</p>
<p>In the above, it is for error-free <span class="math inline">\(y\)</span>. Suppose there is error in <span class="math inline">\(y\)</span> conditioning on <span class="math inline">\(x\)</span>, for instance, <span class="math inline">\(y = x\beta + \epsilon, \epsilon \sim N(0, \sigma^2)\)</span>. Then the <span class="math inline">\(\epe(x_0)\)</span> (note that here we are not taking the whole <span class="math inline">\(f\)</span> and data space but just a point <span class="math inline">\(x_0\)</span>), we have</p>
<p><span class="math display">\[\begin{aligned}
  \epe(x_0) &amp;= \E_{y_0 | x_0} \E_\tdata[y_0 - \hat{y}_0]^2 \\
  &amp;= \cdots \\
  &amp;= \sigma^2 + \var_\tdata(\hat{y}_0) + \underbrace{\bias^2(\hat{y}_0)}_0 \\
  &amp;= \sigma^2 + \E_\tdata x_0 (X&#39;X)^{-1} x_0 \sigma^2 \\
  \E_{x_0}[\epe(x_0)] &amp;= \cdots = \sigma^2 (p / N) + \sigma^2
\end{aligned}\]</span></p>
<p>The extra <span class="math inline">\(\sigma^2\)</span> term beside Var and Bias is from the fact that <span class="math inline">\(y\)</span> is not noise free. And this <span class="math inline">\(\sigma^2\)</span> also goes into knn. Here the bias is zero since the model is unbiased.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
