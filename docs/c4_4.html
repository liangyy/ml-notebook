<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Chapter 4: 4.4</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning Notebook</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="esl.html">ESL reading</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/liangyy/ml-notebook">
    <span class="fas fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Chapter 4: 4.4</h1>
<h4 class="date">03 January, 2021</h4>

</div>


<p><span class="math display">\[
\newcommand{\epe}{\text{EPE}}
\newcommand{\E}{\text{E}}
\newcommand{\mse}{\text{MSE}}
\newcommand{\tdata}{\mathcal{T}}
\newcommand{\bias}{\text{Bias}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\corr}{\text{Corr}}
\newcommand{\rss}{\text{RSS}}
\newcommand{\tr}{\text{tr}}
\newcommand{\ridge}{\text{ridge}}
\newcommand{\pcr}{\text{pcr}}
\newcommand{\ls}{\text{ls}}
\newcommand{\tr}{\text{tr}}
\newcommand{\df}{\text{df}}
\newcommand{\FS}{\text{FS}}
\newcommand{\wt}{\widetilde}
\]</span> <strong>Logistic regression</strong></p>
<div id="overview" class="section level1">
<h1><span class="header-section-number">1</span> Overview</h1>
<p>The definition is <span class="math display">\[\log \frac{\Pr(G = k | X = x)}{\Pr(G = K | X = x)} = \beta_{k}&#39; x, ~\forall k \in \{1, \cdots, K - 1\}\]</span> Note that different choices of <span class="math inline">\(K\)</span> give equivalent results. And the model ensures that <span class="math inline">\(\sum_{k = 1}^K \Pr(G = k | X = x) = 1\)</span>.</p>
<p>Let use <span class="math inline">\(p_k(x; \theta)\)</span> to represent <span class="math inline">\(\Pr(G = k | X = x)\)</span>.</p>
</div>
<div id="fitting-the-logistic-regression" class="section level1">
<h1><span class="header-section-number">2</span> Fitting the logistic regression</h1>
<p>The log-likelihood is <span class="math display">\[l(\theta) = \sum_{i = 1}^N p_{g_i}(x_i; \theta)\]</span>. In the case of binary classification (<span class="math inline">\(K = 2\)</span>), the equation is relatively simple. For <span class="math inline">\(K = 2\)</span>, the score function is: <span class="math display">\[\begin{aligned}
  \frac{\partial l(\beta)}{\partial \beta} &amp;= \sum_i x_i(y_i - p(x_i; \beta))
\end{aligned}\]</span> By setting the derivative to zero, we have <span class="math inline">\(p+1\)</span> nonlinear equations. Note that we include intercept implicitly.</p>
<p>To solve for the score function equal to zero, we can use Newton-Raphson which also needs the Hessian. <span class="math display">\[\begin{aligned}
  \frac{\partial^2 l(\beta)}{\partial \beta partial \beta&#39;} &amp;= -\sum_i x_i x_i&#39; p(x_i; \beta)(1 - p(x_i; \beta))
\end{aligned}\]</span> From here we can derive the update rule: <span class="math display">\[\begin{aligned}
  \beta^{\text{new}} &amp;= (X&#39; W X)^{-1} X&#39; W z \\
  z &amp;= X\beta^{\text{old}} + W^{-1} (y - p) \\
  W &amp;= p(x; \beta^{\text{old}}) (1 - p(x; \beta^{\text{old}}))
\end{aligned}\]</span> This algorithm is called iteratively reweighted least squares (IRLS). Usually, <span class="math inline">\(\beta = 0\)</span> is a good initial point but it does not guarantee to converge. For multi-class models, the IRLS algorithm is still applicable but the derivation is more complicated.</p>
<p>Alternatively, we can also use coordinate descent to solve the logistic model. The <code>glmnet</code> package can fit large problem (both in <span class="math inline">\(p\)</span> and <span class="math inline">\(N\)</span>). In practice, logistic models can be used to explore the relation between the outcome variable and the input features.</p>
</div>
<div id="an-example" class="section level1">
<h1><span class="header-section-number">3</span> An example</h1>
<p>Here we fit the logistic model with multiple variables to study the risk factor of myocardial infarctions. We used Wald test statistic to determine if a variable is significant contributing to the outcome risk. Note that with multiple correlated variables, we should interpret the sign of the slope with cautious.</p>
</div>
<div id="quadratic-approximations-and-inference" class="section level1">
<h1><span class="header-section-number">4</span> Quadratic approximations and inference</h1>
<p>It’s been pointed out that the mle of the logistic model <span class="math inline">\(\hat\beta\)</span> is also the weighted least squares coefficients of the responce <span class="math inline">\(z_i = x_i&#39; \hat\beta + \frac{y_i - \hat{p}_i}{\hat{p}_i ( 1 - \hat{p}_i )}\)</span> with weight <span class="math inline">\(w_i = \hat{p}_i ( 1 - \hat{p}_i )\)</span>.</p>
<p>Related to the above relation, there are some useful results:</p>
<ul>
<li>The weighted RSS is the familiar Pearson chi-squares statistic <span class="math inline">\(\sum_i \frac{(y_i - \hat{p}_i)^2}{\hat{p}_i(1 - \hat{p}_i)}\)</span>.</li>
<li>Asymptotic likelihood theory, if the model is correct, <span class="math inline">\(\hat\beta\)</span> is consistent.</li>
<li>By CLT, the distribution of <span class="math inline">\(\hat\beta\)</span> converges to <span class="math inline">\(N(\beta, (X&#39; W X)^{-1})\)</span>.</li>
<li>The model fitting is time-consuming. Popular shortcuts are the Rao score test and the Wald test which include or exclude a variable from the current model which are both only based on the current mle fit. So, they are computationally efficient. Some software implementations have taken advantage of these such as <code>glm</code> in R.</li>
</ul>
</div>
<div id="l_1-regularized-logistic-regression" class="section level1">
<h1><span class="header-section-number">5</span> <span class="math inline">\(L_1\)</span> regularized logistic regression</h1>
<p>Similar to the linear model, we can add the lasso penalty to the model <span class="math inline">\(\lambda \sum_j |\beta_j|\)</span>. The problem is still convex and we can use non-linear programming methods to solve it. Alternative, we can use the quadratic approximations as shown in the logistic regression with Newton’s method. Here, we can fit the weighted least squares with the lasso penalty instead. Interestingly, the score equation for the non-zero <span class="math inline">\(\beta_j\)</span> is <span class="math inline">\(x_j&#39; (y - p) = \lambda \cdot \text{sign}(\beta_j)\)</span> The path based algorithms (by LAR) are still applicable but it is more difficult than the linear model since the path is not linear anymore (<em>e.g.</em> <code>glmpath</code> in R using predictor-corrector methods). The coordinate descent methods are also applicable, <em>e.g.</em> <code>glmnet</code> which is very efficient and affordable for large problem (large in <span class="math inline">\(N\)</span> or <span class="math inline">\(p\)</span>).</p>
</div>
<div id="logistic-regression-or-lda" class="section level1">
<h1><span class="header-section-number">6</span> logistic regression or LDA?</h1>
<p>Under the assumption of Gaussian model and common covariance, the logistic regression and the LDA method have the same expression for <span class="math inline">\(\log \frac{\Pr(G = k | X = x)}{\Pr(G = K | X = x)}\)</span> which is a linear function in <span class="math inline">\(x\)</span>. But they differ by how to estimate the model parameters.</p>
<p>For the logistic model, we can think of it as looking for parameters to optimize <span class="math inline">\(\Pr(X = x) \Pr(G = k | X = x)\)</span> where we have explicit expression for the second term (the conditional probability) and implicitly we assign the same weight to each of <span class="math inline">\(X = x\)</span> in the training data. So, we essentially fit <span class="math inline">\(\Pr(X = x)\)</span> non-parametrically.</p>
<p>For LDA, we fit the model parameters by maximizing <span class="math inline">\(Pr(G = k, X = x) = \phi(X = x; \mu_k, \Sigma_k)\pi_k = \Pr(X = x | G = k) \pi_k\)</span> where <span class="math inline">\(\phi(\cdot)\)</span> is the Gaussian density. So, here <span class="math inline">\(\Pr(X = x) = \sum_k \phi(X = x; \mu_k, \Sigma_k) \pi_k\)</span> and we fit this distribution explicitly.</p>
<p>From these results it seems that the logistic model makes fewer assumptions that the LDA. With this extra assumption, we have more information to be used to estimate model parameters (which means lower variance). So, <span class="math inline">\(f_k(x)\)</span> is indeed Gaussian, the LDA is more efficient than the logistic model by 30% (LDA does as well as the logistic when the latter has 30% more data).</p>
<p>In practice, on one hand, the common covariance means that even the points very far from the decision boundaries will contribute to the estimation of covariance and so does the outliers. But on the other hand, in the LDA model, we can also utilize the unlabelled points in estimating <span class="math inline">\(\Pr(X = x)\)</span>.</p>
<p>In fact, we could think of the additional assumptions on the marginal distribution as a regularization. For instance, for perfectly separable data, the logistic model (the mle) is not well-defined (we have infinite number of solutions) but the LDA is well-defined in this case.</p>
<p>In practice, with many qualitative variables, the Gaussian assumption is less likely to be true and people will prefer the logistic model as a safer choice. But in fact, the logistic model and the LDA have very similar performance even on qualitative features.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
